{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zdLEmhpeXfX5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import argparse\n",
        "from scipy.stats import poisson\n",
        "from skimage.transform import rescale, resize\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import itertools\n",
        "from torchvision import transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DataSet**"
      ],
      "metadata": {
        "id": "y4ejBVsgfaYR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_dir, transform=None, task=None, data_type='both'):\n",
        "        self.data_dir_a = data_dir + 'A'\n",
        "        self.data_dir_b = data_dir + 'B'\n",
        "        self.transform = transform\n",
        "        self.task = task\n",
        "        self.data_type = data_type\n",
        "\n",
        "\n",
        "        self.to_tensor = ToTensor()\n",
        "\n",
        "        if os.path.exists(self.data_dir_a):\n",
        "            lst_data_a = os.listdir(self.data_dir_a)\n",
        "            lst_data_a = [f for f in lst_data_a if f.endswith('jpg') | f.endswith('jpeg') | f.endswith('png')]\n",
        "            lst_data_a.sort()\n",
        "        else:\n",
        "            lst_data_a = []\n",
        "\n",
        "        if os.path.exists(self.data_dir_b):\n",
        "            lst_data_b = os.listdir(self.data_dir_b)\n",
        "            lst_data_b = [f for f in lst_data_b if f.endswith('jpg') | f.endswith('jpeg') | f.endswith('png')]\n",
        "            lst_data_b.sort()\n",
        "        else:\n",
        "            lst_data_b = []\n",
        "\n",
        "        self.lst_data_a = lst_data_a\n",
        "        self.lst_data_b = lst_data_b\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.data_type == 'both':\n",
        "            if len(self.lst_data_a) < len(self.lst_data_b):\n",
        "                return len(self.lst_data_a)\n",
        "            else:\n",
        "                return len(self.lst_data_b)\n",
        "        elif self.data_type == 'a':\n",
        "            return len(self.lst_data_a)\n",
        "        elif self.data_type == 'b':\n",
        "            return len(self.lst_data_b)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        data = {}\n",
        "        if self.data_type == 'a' or self.data_type == 'both':\n",
        "            data_a = plt.imread(os.path.join(self.data_dir_a, self.lst_data_a[index]))[:, :, :3]\n",
        "\n",
        "            if data_a.ndim == 2:\n",
        "                data_a = data_a[:, :, np.newaxis]\n",
        "            if data_a.dtype == np.uint8:\n",
        "                data_a = data_a / 255.0\n",
        "\n",
        "            # data = {'data_a': data_a}\n",
        "            data['data_a'] = data_a\n",
        "\n",
        "        if self.data_type == 'b' or self.data_type == 'both':\n",
        "\n",
        "            data_b = plt.imread(os.path.join(self.data_dir_b, self.lst_data_b[index]))[:, :, :3]\n",
        "\n",
        "            if data_b.ndim == 2:\n",
        "                data_b = data_b[:, :, np.newaxis]\n",
        "            if data_b.dtype == np.uint8:\n",
        "                data_b = data_b / 255.0\n",
        "\n",
        "            # data = {'data_b': data_b}\n",
        "            data['data_b'] = data_b\n",
        "\n",
        "        if self.transform:\n",
        "            data = self.transform(data)\n",
        "\n",
        "        data = self.to_tensor(data)\n",
        "\n",
        "        return data"
      ],
      "metadata": {
        "id": "X6ZFeMywfieY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ToTensor(object):\n",
        "    def __call__(self, data):\n",
        "        # label, input = data['label'], data['input']\n",
        "        #\n",
        "        # label = label.transpose((2, 0, 1)).astype(np.float32)\n",
        "        # input = input.transpose((2, 0, 1)).astype(np.float32)\n",
        "        #\n",
        "        # data = {'label': torch.from_numpy(label), 'input': torch.from_numpy(input)}\n",
        "\n",
        "        # Updated at Apr 5 2020\n",
        "        for key, value in data.items():\n",
        "            value = value.transpose((2, 0, 1)).astype(np.float32)\n",
        "            data[key] = torch.from_numpy(value)\n",
        "\n",
        "        return data"
      ],
      "metadata": {
        "id": "wm9J0vXcfm46"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Normalization(object):\n",
        "    def __init__(self, mean=0.5, std=0.5):\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __call__(self, data):\n",
        "        # label, input = data['label'], data['input']\n",
        "        #\n",
        "        # input = (input - self.mean) / self.std\n",
        "        # label = (label - self.mean) / self.std\n",
        "        #\n",
        "        # data = {'label': label, 'input': input}\n",
        "\n",
        "        # Updated at Apr 5 2020\n",
        "        for key, value in data.items():\n",
        "            data[key] = (value - self.mean) / self.std\n",
        "\n",
        "        return data"
      ],
      "metadata": {
        "id": "YHG9YPD3fpLW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomFlip(object):\n",
        "    def __call__(self, data):\n",
        "        # label, input = data['label'], data['input']\n",
        "\n",
        "        if np.random.rand() > 0.5:\n",
        "            # label = np.fliplr(label)\n",
        "            # input = np.fliplr(input)\n",
        "\n",
        "            # Updated at Apr 5 2020\n",
        "            for key, value in data.items():\n",
        "                data[key] = np.flip(value, axis=0)\n",
        "\n",
        "        if np.random.rand() > 0.5:\n",
        "            # label = np.flipud(label)\n",
        "            # input = np.flipud(input)\n",
        "\n",
        "            # Updated at Apr 5 2020\n",
        "            for key, value in data.items():\n",
        "                data[key] = np.flip(value, axis=1)\n",
        "\n",
        "        # data = {'label': label, 'input': input}\n",
        "\n",
        "        return data"
      ],
      "metadata": {
        "id": "ujt_44ejfrLp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomCrop(object):\n",
        "  def __init__(self, shape):\n",
        "      self.shape = shape\n",
        "\n",
        "  def __call__(self, data):\n",
        "    # input, label = data['input'], data['label']\n",
        "    # h, w = input.shape[:2]\n",
        "\n",
        "    keys = list(data.keys())\n",
        "\n",
        "    h, w = data[keys[0]].shape[:2]\n",
        "    new_h, new_w = self.shape\n",
        "\n",
        "    top = np.random.randint(0, h - new_h)\n",
        "    left = np.random.randint(0, w - new_w)\n",
        "\n",
        "    id_y = np.arange(top, top + new_h, 1)[:, np.newaxis]\n",
        "    id_x = np.arange(left, left + new_w, 1)\n",
        "\n",
        "    # input = input[id_y, id_x]\n",
        "    # label = label[id_y, id_x]\n",
        "    # data = {'label': label, 'input': input}\n",
        "\n",
        "    # Updated at Apr 5 2020\n",
        "    for key, value in data.items():\n",
        "        data[key] = value[id_y, id_x]\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "Kn0IcOC5ftTC"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Resize(object):\n",
        "    def __init__(self, shape):\n",
        "        self.shape = shape\n",
        "\n",
        "    def __call__(self, data):\n",
        "        for key, value in data.items():\n",
        "            data[key] = resize(value, output_shape=(self.shape[0], self.shape[1],\n",
        "                                                    self.shape[2]))\n",
        "\n",
        "        return data"
      ],
      "metadata": {
        "id": "cjlweYlGfvEp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Utils**"
      ],
      "metadata": {
        "id": "TAAUk6FZcGW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def set_requires_grad(nets, requires_grad=False):\n",
        "    \"\"\"Set requies_grad=Fasle for all the networks to avoid unnecessary computations\n",
        "    Parameters:\n",
        "        nets (network list)   -- a list of networks\n",
        "        requires_grad (bool)  -- whether the networks require gradients or not\n",
        "    \"\"\"\n",
        "    if not isinstance(nets, list):\n",
        "        nets = [nets]\n",
        "    for net in nets:\n",
        "        if net is not None:\n",
        "            for param in net.parameters():\n",
        "                param.requires_grad = requires_grad"
      ],
      "metadata": {
        "id": "6H19rLTycL2U"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_weights(net, init_type='normal', init_gain=0.02):\n",
        "    \"\"\"Initialize network weights.\n",
        "\n",
        "    Parameters:\n",
        "        net (network)   -- network to be initialized\n",
        "        init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
        "        init_gain (float)    -- scaling factor for normal, xavier and orthogonal.\n",
        "\n",
        "    We use 'normal' in the original pix2pix and CycleGAN paper. But xavier and kaiming might\n",
        "    work better for some applications. Feel free to try yourself.\n",
        "    \"\"\"\n",
        "    def init_func(m):  # define the initialization function\n",
        "        classname = m.__class__.__name__\n",
        "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
        "            if init_type == 'normal':\n",
        "                nn.init.normal_(m.weight.data, 0.0, init_gain)\n",
        "            elif init_type == 'xavier':\n",
        "                nn.init.xavier_normal_(m.weight.data, gain=init_gain)\n",
        "            elif init_type == 'kaiming':\n",
        "                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
        "            elif init_type == 'orthogonal':\n",
        "                nn.init.orthogonal_(m.weight.data, gain=init_gain)\n",
        "            else:\n",
        "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
        "            if hasattr(m, 'bias') and m.bias is not None:\n",
        "                nn.init.constant_(m.bias.data, 0.0)\n",
        "        elif classname.find('BatchNorm2d') != -1:  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n",
        "            nn.init.normal_(m.weight.data, 1.0, init_gain)\n",
        "            nn.init.constant_(m.bias.data, 0.0)\n",
        "\n",
        "    print('initialize network with %s' % init_type)\n",
        "    net.apply(init_func)  # apply the initialization function <init_func>"
      ],
      "metadata": {
        "id": "Jz-IqENbcQ70"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save(ckpt_dir, netG_a2b, netG_b2a, netD_a, netD_b, optimG, optimD, epoch):\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        os.makedirs(ckpt_dir)\n",
        "\n",
        "    torch.save({'netG_a2b': netG_a2b.state_dict(), 'netG_b2a': netG_b2a.state_dict(),\n",
        "                'netD_a': netD_a.state_dict(), 'netD_b': netD_b.state_dict(),\n",
        "                'optimG': optimG.state_dict(), 'optimD': optimD.state_dict()},\n",
        "               \"%s/model_epoch%d.pth\" % (ckpt_dir, epoch))"
      ],
      "metadata": {
        "id": "G_dIJwIKcTtT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load(ckpt_dir, netG_a2b, netG_b2a, netD_a, netD_b, optimG, optimD):\n",
        "    if not os.path.exists(ckpt_dir):\n",
        "        epoch = 0\n",
        "        return netG_a2b, netG_b2a, netD_a, netD_b, optimG, optimD, epoch\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    ckpt_lst = os.listdir(ckpt_dir)\n",
        "    ckpt_lst = [f for f in ckpt_lst if f.endswith('pth')]\n",
        "    ckpt_lst.sort(key=lambda f: int(''.join(filter(str.isdigit, f))))\n",
        "\n",
        "    dict_model = torch.load('%s/%s' % (ckpt_dir, ckpt_lst[-1]), map_location=device)\n",
        "\n",
        "    netG_a2b.load_state_dict(dict_model['netG_a2b'])\n",
        "    netG_b2a.load_state_dict(dict_model['netG_b2a'])\n",
        "    netD_a.load_state_dict(dict_model['netD_a'])\n",
        "    netD_b.load_state_dict(dict_model['netD_b'])\n",
        "    optimG.load_state_dict(dict_model['optimG'])\n",
        "    optimD.load_state_dict(dict_model['optimD'])\n",
        "    epoch = int(ckpt_lst[-1].split('epoch')[1].split('.pth')[0])\n",
        "\n",
        "    return netG_a2b, netG_b2a, netD_a, netD_b, optimG, optimD, epoch"
      ],
      "metadata": {
        "id": "GUCzYwfYcWqN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Add Sampling\n",
        "def add_sampling(img, type=\"random\", opts=None):\n",
        "    sz = img.shape\n",
        "\n",
        "    if type == \"uniform\":\n",
        "        ds_y = opts[0].astype(np.int)\n",
        "        ds_x = opts[1].astype(np.int)\n",
        "\n",
        "        msk = np.zeros(img.shape)\n",
        "        msk[::ds_y, ::ds_x, :] = 1\n",
        "\n",
        "        dst = img * msk\n",
        "\n",
        "    elif type == \"random\":\n",
        "        prob = opts[0]\n",
        "\n",
        "        # rnd = np.random.rand(sz[0], sz[1], 1)\n",
        "        # msk = (rnd < prob).astype(np.float)\n",
        "        # msk = np.tile(msk, (1, 1, sz[2]))\n",
        "\n",
        "        rnd = np.random.rand(sz[0], sz[1], sz[2])\n",
        "        msk = (rnd > prob).astype(np.float)\n",
        "\n",
        "        dst = img * msk\n",
        "\n",
        "    elif type == \"gaussian\":\n",
        "        x0 = opts[0]\n",
        "        y0 = opts[1]\n",
        "        sgmx = opts[2]\n",
        "        sgmy = opts[3]\n",
        "\n",
        "        a = opts[4]\n",
        "\n",
        "        ly = np.linspace(-1, 1, sz[0])\n",
        "        lx = np.linspace(-1, 1, sz[1])\n",
        "\n",
        "        x, y = np.meshgrid(lx, ly)\n",
        "\n",
        "        gaus = a * np.exp(-((x - x0)**2/(2*sgmx**2) + (y - y0)**2/(2*sgmy**2)))\n",
        "        gaus = np.tile(gaus[:, :, np.newaxis], (1, 1, sz[2]))\n",
        "        rnd = np.random.rand(sz[0], sz[1], sz[2])\n",
        "        msk = (rnd < gaus).astype(np.float)\n",
        "\n",
        "        # gaus = a * np.exp(-((x - x0) ** 2 / (2 * sgmx ** 2) + (y - y0) ** 2 / (2 * sgmy ** 2)))\n",
        "        # gaus = np.tile(gaus[:, :, np.newaxis], (1, 1, 1))\n",
        "        # rnd = np.random.rand(sz[0], sz[1], 1)\n",
        "        # msk = (rnd < gaus).astype(np.float)\n",
        "        # msk = np.tile(msk, (1, 1, sz[2]))\n",
        "\n",
        "        dst = img * msk\n",
        "\n",
        "    return dst"
      ],
      "metadata": {
        "id": "_mB6XqhUcbJE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Add Noise\n",
        "def add_noise(img, type=\"random\", opts=None):\n",
        "    sz = img.shape\n",
        "\n",
        "    if type == \"random\":\n",
        "        sgm = opts[0]\n",
        "\n",
        "        noise = sgm / 255.0 * np.random.randn(sz[0], sz[1], sz[2])\n",
        "\n",
        "        dst = img + noise\n",
        "\n",
        "    elif type == \"poisson\":\n",
        "        dst = poisson.rvs(255.0 * img) / 255.0\n",
        "        noise = dst - img\n",
        "\n",
        "    return dst"
      ],
      "metadata": {
        "id": "p5QkgQQ3cdSU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Add blurring\n",
        "def add_blur(img, type=\"bilinear\", opts=None):\n",
        "    if type == \"nearest\":\n",
        "        order = 0\n",
        "    elif type == \"bilinear\":\n",
        "        order = 1\n",
        "    elif type == \"biquadratic\":\n",
        "        order = 2\n",
        "    elif type == \"bicubic\":\n",
        "        order = 3\n",
        "    elif type == \"biquartic\":\n",
        "        order = 4\n",
        "    elif type == \"biquintic\":\n",
        "        order = 5\n",
        "\n",
        "    sz = img.shape\n",
        "    if len(opts) == 1:\n",
        "        keepdim = True\n",
        "    else:\n",
        "        keepdim = opts[1]\n",
        "\n",
        "    # dw = 1.0 / opts[0]\n",
        "    # dst = rescale(img, scale=(dw, dw, 1), order=order)\n",
        "    dst = resize(img, output_shape=(sz[0] // opts[0], sz[1] // opts[0], sz[2]), order=order)\n",
        "\n",
        "    if keepdim:\n",
        "        # dst = rescale(dst, scale=(1 / dw, 1 / dw, 1), order=order)\n",
        "        dst = resize(dst, output_shape=(sz[0], sz[1], sz[2]), order=order)\n",
        "\n",
        "    return dst"
      ],
      "metadata": {
        "id": "_iW_mIvjcjTj"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "t8ij5-x7cmrL"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def patch2image(src, nimg, npatch, nmargin, datatype=\"tensor\", type=\"count\"):\n",
        "    src = src.to('cpu').detach().numpy()\n",
        "\n",
        "    nimg_zp = np.zeros(4, np.int32)\n",
        "    ncrop = np.zeros(4, np.int32)\n",
        "    nset = np.zeros(4, np.int32)\n",
        "\n",
        "    for id in range(0, 4):\n",
        "        nimg_zp[id] = int(nimg[id] + 2 * nmargin[id])\n",
        "        ncrop[id] = int(npatch[id] - 2 * nmargin[id])\n",
        "        nset[id] = np.ceil(nimg_zp[id] / ncrop[id]).astype(np.int32)\n",
        "\n",
        "    nsmp = np.prod(nset)\n",
        "\n",
        "    iset = [(np.linspace(0, nimg_zp[0] - npatch[0], nset[0])).astype(np.int32),\n",
        "             (np.linspace(0, nimg_zp[1] - npatch[1], nset[1])).astype(np.int32),\n",
        "             (np.linspace(0, nimg_zp[2] - npatch[2], nset[2])).astype(np.int32),\n",
        "             (np.linspace(0, nimg_zp[3] - npatch[3], nset[3])).astype(np.int32)]\n",
        "\n",
        "    crop = [nmargin[0] + np.arange(0, ncrop[0])[:, np.newaxis, np.newaxis, np.newaxis],\n",
        "            nmargin[1] + np.arange(0, ncrop[1])[:, np.newaxis, np.newaxis],\n",
        "            nmargin[2] + np.arange(0, ncrop[2])[:, np.newaxis],\n",
        "            nmargin[3] + np.arange(0, ncrop[3])]\n",
        "\n",
        "    dst = np.zeros([nimg_zp[0], nimg_zp[1], nimg_zp[2], nimg_zp[3]], dtype=np.float32)\n",
        "    wgt = np.zeros([nimg_zp[0], nimg_zp[1], nimg_zp[2], nimg_zp[3]], dtype=np.float32)\n",
        "\n",
        "    i_img = [np.arange(nmargin[0], nimg_zp[0] - nmargin[0]).astype(np.int32)[:, np.newaxis, np.newaxis, np.newaxis],\n",
        "             np.arange(nmargin[1], nimg_zp[1] - nmargin[1]).astype(np.int32)[:, np.newaxis, np.newaxis],\n",
        "             np.arange(nmargin[2], nimg_zp[2] - nmargin[2]).astype(np.int32)[:, np.newaxis],\n",
        "             np.arange(nmargin[3], nimg_zp[3] - nmargin[3]).astype(np.int32)]\n",
        "\n",
        "    bnd = [ncrop[0] - iset[0][1] if not len(iset[0]) == 1 else 0,\n",
        "           ncrop[1] - iset[1][1] if not len(iset[1]) == 1 else 0,\n",
        "           ncrop[2] - iset[2][1] if not len(iset[2]) == 1 else 0,\n",
        "           ncrop[3] - iset[3][1] if not len(iset[3]) == 1 else 0]\n",
        "\n",
        "    if type == 'cos':\n",
        "        wgt_bnd = [None for _ in range(4)]\n",
        "\n",
        "        for id in range(1, 4):\n",
        "            t = np.linspace(np.pi, 2 * np.pi, bnd[id])\n",
        "            wgt_ = np.ones((ncrop[id]), np.float32)\n",
        "            wgt_[0:bnd[id]] = (np.cos(t) + 1.0)/2.0\n",
        "\n",
        "            axis_ = [f for f in range(0, 4)]\n",
        "            axis_.remove(id)\n",
        "            wgt_ = np.expand_dims(wgt_, axis=axis_)\n",
        "\n",
        "            ncrop_ = [ncrop[f] for f in range(0, 4)]\n",
        "            ncrop_[id] = 1\n",
        "\n",
        "            wgt_bnd[id] = np.tile(wgt_, ncrop_)\n",
        "\n",
        "    for i in range(0, nset[0]):\n",
        "        for j in range(0, nset[1]):\n",
        "            for k in range(0, nset[2]):\n",
        "                for q in range(0, nset[3]):\n",
        "\n",
        "                    wgt_ = np.ones(ncrop, np.float32)\n",
        "\n",
        "                    if type == 'cos':\n",
        "                        for id in range(1, 4):\n",
        "                            if id == 1:\n",
        "                                axs = j\n",
        "                            elif id == 2:\n",
        "                                axs = k\n",
        "                            elif id == 3:\n",
        "                                axs = q\n",
        "\n",
        "                            if axs == 0:\n",
        "                                wgt_ *= np.flip(wgt_bnd[id], id)\n",
        "                            elif axs == nset[id] - 1:\n",
        "                                wgt_ *= wgt_bnd[id]\n",
        "                            else:\n",
        "                                wgt_ *= np.flip(wgt_bnd[id], id) * wgt_bnd[id]\n",
        "\n",
        "                    pos = [nset[3] * nset[2] * nset[1] * i + nset[2] * nset[1] * j + nset[1] * k + q]\n",
        "\n",
        "                    i_ = iset[0][i] + crop[0]\n",
        "                    j_ = iset[1][j] + crop[1]\n",
        "                    k_ = iset[2][k] + crop[2]\n",
        "                    q_ = iset[3][q] + crop[3]\n",
        "\n",
        "                    src_ = src[pos, :, :, :]\n",
        "                    dst[i_, j_, k_, q_] = dst[i_, j_, k_, q_] + src_[crop[0], crop[1], crop[2], crop[3]] * wgt_\n",
        "                    wgt[i_, j_, k_, q_] = wgt[i_, j_, k_, q_] + wgt_\n",
        "\n",
        "    if type == 'count':\n",
        "        dst = dst/wgt\n",
        "\n",
        "    dst = dst[i_img[0], i_img[1], i_img[2], i_img[3]]\n",
        "    wgt = wgt[i_img[0], i_img[1], i_img[2], i_img[3]]\n",
        "\n",
        "    if datatype == \"tensor\":\n",
        "        dst = torch.from_numpy(dst)\n",
        "        wgt = torch.from_numpy(wgt)\n",
        "\n",
        "    return dst"
      ],
      "metadata": {
        "id": "LPPcesnscrNz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Layer**"
      ],
      "metadata": {
        "id": "3QPCwwI5c2vr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DECBR2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1, bias=True, norm=\"bnorm\", relu=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "        # layers += [nn.ReflectionPad2d(padding=padding)]\n",
        "        layers += [nn.ConvTranspose2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                                      kernel_size=kernel_size, stride=stride, padding=padding, output_padding=output_padding,\n",
        "                                      bias=bias)]\n",
        "\n",
        "        if not norm is None:\n",
        "            if norm == \"bnorm\":\n",
        "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
        "            elif norm == \"inorm\":\n",
        "                layers += [nn.InstanceNorm2d(num_features=out_channels)]\n",
        "\n",
        "        if not relu is None and relu >= 0.0:\n",
        "            layers += [nn.ReLU() if relu == 0 else nn.LeakyReLU(relu)]\n",
        "\n",
        "        self.cbr = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.cbr(x)"
      ],
      "metadata": {
        "id": "YgoNata2c-mu"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CBR2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, padding_mode='reflection', bias=True, norm=\"bnorm\", relu=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        if padding_mode == 'reflection':\n",
        "            layers += [nn.ReflectionPad2d(padding)]\n",
        "        elif padding_mode == 'replication':\n",
        "            layers += [nn.ReplicationPad2d(padding)]\n",
        "        elif padding_mode == 'constant':\n",
        "            value = 0\n",
        "            layers += [nn.ConstantPad2d(padding, value)]\n",
        "        elif padding_mode == 'zeros':\n",
        "            layers += [nn.ZeroPad2d(padding)]\n",
        "\n",
        "        layers += [nn.Conv2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                             kernel_size=kernel_size, stride=stride, padding=0,\n",
        "                             bias=bias)]\n",
        "\n",
        "        if not norm is None:\n",
        "            if norm == \"bnorm\":\n",
        "                layers += [nn.BatchNorm2d(num_features=out_channels)]\n",
        "            elif norm == \"inorm\":\n",
        "                layers += [nn.InstanceNorm2d(num_features=out_channels)]\n",
        "\n",
        "        if not relu is None and relu >= 0.0:\n",
        "            layers += [nn.ReLU() if relu == 0 else nn.LeakyReLU(relu)]\n",
        "\n",
        "        self.cbr = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.cbr(x)"
      ],
      "metadata": {
        "id": "-xOW3epedBZL"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=True, norm=\"bnorm\", relu=0.0):\n",
        "        super().__init__()\n",
        "\n",
        "        layers = []\n",
        "\n",
        "        # 1st conv\n",
        "        layers += [CBR2d(in_channels=in_channels, out_channels=out_channels,\n",
        "                         kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                         bias=bias, norm=norm, relu=relu)]\n",
        "\n",
        "        # 2nd conv\n",
        "        layers += [CBR2d(in_channels=out_channels, out_channels=out_channels,\n",
        "                         kernel_size=kernel_size, stride=stride, padding=padding,\n",
        "                         bias=bias, norm=norm, relu=None)]\n",
        "\n",
        "        self.resblk = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.resblk(x)"
      ],
      "metadata": {
        "id": "COu4lHOUdDlF"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PixelUnshuffle(nn.Module):\n",
        "    def __init__(self, ry=2, rx=2):\n",
        "        super().__init__()\n",
        "        self.ry = ry\n",
        "        self.rx = rx\n",
        "\n",
        "    def forward(self, x):\n",
        "        ry = self.ry\n",
        "        rx = self.rx\n",
        "\n",
        "        [B, C, H, W] = list(x.shape)\n",
        "\n",
        "        x = x.reshape(B, C, H // ry, ry, W // rx, rx)\n",
        "        x = x.permute(0, 1, 3, 5, 2, 4)\n",
        "        x = x.reshape(B, C * (ry * rx), H // ry, W // rx)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "q-4ry4bpdFwD"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PixelShuffle(nn.Module):\n",
        "    def __init__(self, ry=2, rx=2):\n",
        "        super().__init__()\n",
        "        self.ry = ry\n",
        "        self.rx = rx\n",
        "\n",
        "    def forward(self, x):\n",
        "        ry = self.ry\n",
        "        rx = self.rx\n",
        "\n",
        "        [B, C, H, W] = list(x.shape)\n",
        "\n",
        "        x = x.reshape(B, C // (ry * rx), ry, rx, H, W)\n",
        "        x = x.permute(0, 1, 4, 2, 5, 3)\n",
        "        x = x.reshape(B, C // (ry * rx), H * ry, W * rx)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "YSIQi8DSdHcr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Models**"
      ],
      "metadata": {
        "id": "_0riY-FNdWXz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.CycleGan**"
      ],
      "metadata": {
        "id": "k-gCqUBwdcg9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/pdf/1703.10593"
      ],
      "metadata": {
        "id": "fJ9ppTondjzq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CycleGAN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, nker=64, norm='bnorm', nblk=6):\n",
        "        super(CycleGAN, self).__init__()\n",
        "\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.nker = nker\n",
        "        self.norm = norm\n",
        "        self.nblk = nblk\n",
        "\n",
        "        if norm == 'bnorm':\n",
        "            self.bias = False\n",
        "        else:\n",
        "            self.bias = True\n",
        "\n",
        "        self.enc1 = CBR2d(self.in_channels, 1 * self.nker, kernel_size=7, stride=1, padding=3, norm=self.norm, relu=0.0)\n",
        "        self.enc2 = CBR2d(1 * self.nker, 2 * self.nker, kernel_size=3, stride=2, padding=1, norm=self.norm, relu=0.0)\n",
        "        self.enc3 = CBR2d(2 * self.nker, 4 * self.nker, kernel_size=3, stride=2, padding=1, norm=self.norm, relu=0.0)\n",
        "\n",
        "        if self.nblk:\n",
        "            res = []\n",
        "\n",
        "            for i in range(self.nblk):\n",
        "                res += [ResBlock(4 * self.nker, 4 * self.nker, kernel_size=3, stride=1, padding=1, norm=self.norm, relu=0.0)]\n",
        "\n",
        "            self.res = nn.Sequential(*res)\n",
        "\n",
        "        self.dec3 = DECBR2d(4 * self.nker, 2 * self.nker, kernel_size=3, stride=2, padding=1, norm=self.norm, relu=0.0)\n",
        "        self.dec2 = DECBR2d(2 * self.nker, 1 * self.nker, kernel_size=3, stride=2, padding=1, norm=self.norm, relu=0.0)\n",
        "        self.dec1 = CBR2d(1 * self.nker, self.out_channels, kernel_size=7, stride=1, padding=3, norm=None, relu=None)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.enc1(x)\n",
        "        x = self.enc2(x)\n",
        "        x = self.enc3(x)\n",
        "\n",
        "        x = self.res(x)\n",
        "\n",
        "        x = self.dec3(x)\n",
        "        x = self.dec2(x)\n",
        "        x = self.dec1(x)\n",
        "\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Xk2yEJiAdbd-"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.Pix2Pix**\n",
        "https://arxiv.org/pdf/1611.07004.pdf"
      ],
      "metadata": {
        "id": "Xl1foXBcdocc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Pix2Pix(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, nker=64, norm=\"bnorm\"):\n",
        "        super(Pix2Pix, self).__init__()\n",
        "\n",
        "        self.enc1 = CBR2d(in_channels, 1 * nker, kernel_size=4, padding=1,\n",
        "                          norm=None, relu=0.2, stride=2)\n",
        "\n",
        "        self.enc2 = CBR2d(1 * nker, 2 * nker, kernel_size=4, padding=1,\n",
        "                          norm=norm, relu=0.2, stride=2)\n",
        "\n",
        "        self.enc3 = CBR2d(2 * nker, 4 * nker, kernel_size=4, padding=1,\n",
        "                          norm=norm, relu=0.2, stride=2)\n",
        "\n",
        "        self.enc4 = CBR2d(4 * nker, 8 * nker, kernel_size=4, padding=1,\n",
        "                          norm=norm, relu=0.2, stride=2)\n",
        "\n",
        "        self.enc5 = CBR2d(8 * nker, 8 * nker, kernel_size=4, padding=1,\n",
        "                          norm=norm, relu=0.2, stride=2)\n",
        "\n",
        "        self.enc6 = CBR2d(8 * nker, 8 * nker, kernel_size=4, padding=1,\n",
        "                          norm=norm, relu=0.2, stride=2)\n",
        "\n",
        "        self.enc7 = CBR2d(8 * nker, 8 * nker, kernel_size=4, padding=1,\n",
        "                          norm=norm, relu=0.2, stride=2)\n",
        "\n",
        "        self.enc8 = CBR2d(8 * nker, 8 * nker, kernel_size=4, padding=1,\n",
        "                          norm=norm, relu=0.2, stride=2)\n",
        "\n",
        "\n",
        "        self.dec1 = DECBR2d(8 * nker, 8 * nker, kernel_size=4, padding=1,\n",
        "                            norm=norm, relu=0.0, stride=2)\n",
        "        self.drop1 = nn.Dropout2d(0.5)\n",
        "\n",
        "        self.dec2 = DECBR2d(2 * 8 * nker, 8 * nker, kernel_size=4, padding=1,\n",
        "                            norm=norm, relu=0.0, stride=2)\n",
        "        self.drop2 = nn.Dropout2d(0.5)\n",
        "\n",
        "        self.dec3 = DECBR2d(2 * 8 * nker, 8 * nker, kernel_size=4, padding=1,\n",
        "                            norm=norm, relu=0.0, stride=2)\n",
        "        self.drop3 = nn.Dropout2d(0.5)\n",
        "\n",
        "        self.dec4 = DECBR2d(2 * 8 * nker, 8 * nker, kernel_size=4, padding=1,\n",
        "                            norm=norm, relu=0.0, stride=2)\n",
        "\n",
        "        self.dec5 = DECBR2d(2 * 8 * nker, 4 * nker, kernel_size=4, padding=1,\n",
        "                            norm=norm, relu=0.0, stride=2)\n",
        "\n",
        "        self.dec6 = DECBR2d(2 * 4 * nker, 2 * nker, kernel_size=4, padding=1,\n",
        "                            norm=norm, relu=0.0, stride=2)\n",
        "\n",
        "        self.dec7 = DECBR2d(2 * 2 * nker, 1 * nker, kernel_size=4, padding=1,\n",
        "                            norm=norm, relu=0.0, stride=2)\n",
        "\n",
        "        self.dec8 = DECBR2d(2 * 1 * nker, out_channels, kernel_size=4, padding=1,\n",
        "                            norm=None, relu=None, stride=2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1 = self.enc1(x)\n",
        "        enc2 = self.enc2(enc1)\n",
        "        enc3 = self.enc3(enc2)\n",
        "        enc4 = self.enc4(enc3)\n",
        "        enc5 = self.enc5(enc4)\n",
        "        enc6 = self.enc6(enc5)\n",
        "        enc7 = self.enc7(enc6)\n",
        "        enc8 = self.enc8(enc7)\n",
        "\n",
        "        dec1 = self.dec1(enc8)\n",
        "        drop1 = self.drop1(dec1)\n",
        "\n",
        "        cat2 = torch.cat((drop1, enc7), dim=1)\n",
        "        dec2 = self.dec2(cat2)\n",
        "        drop2 = self.drop2(dec2)\n",
        "\n",
        "        cat3 = torch.cat((drop2, enc6), dim=1)\n",
        "        dec3 = self.dec3(cat3)\n",
        "        drop3 = self.drop3(dec3)\n",
        "\n",
        "        cat4 = torch.cat((drop3, enc5), dim=1)\n",
        "        dec4 = self.dec4(cat4)\n",
        "\n",
        "        cat5 = torch.cat((dec4, enc4), dim=1)\n",
        "        dec5 = self.dec5(cat5)\n",
        "\n",
        "        cat6 = torch.cat((dec5, enc3), dim=1)\n",
        "        dec6 = self.dec6(cat6)\n",
        "\n",
        "        cat7 = torch.cat((dec6, enc2), dim=1)\n",
        "        dec7 = self.dec7(cat7)\n",
        "\n",
        "        cat8 = torch.cat((dec7, enc1), dim=1)\n",
        "        dec8 = self.dec8(cat8)\n",
        "\n",
        "        x = torch.tanh(dec8)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "qZJKNoQ9dwGd"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.DCGAN**"
      ],
      "metadata": {
        "id": "07df6vlgdxC6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/pdf/1511.06434.pdf"
      ],
      "metadata": {
        "id": "NnHYtgLCd1u6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DCGAN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, nker=64, norm=\"bnorm\"):\n",
        "        super(DCGAN, self).__init__()\n",
        "\n",
        "        self.dec1 = DECBR2d(1 * in_channels, 8 * nker, kernel_size=4, stride=1,\n",
        "                            padding=0, norm=norm, relu=0.0, bias=False)\n",
        "\n",
        "        self.dec2 = DECBR2d(8 * nker, 4 * nker, kernel_size=4, stride=2,\n",
        "                            padding=1, norm=norm, relu=0.0, bias=False)\n",
        "\n",
        "        self.dec3 = DECBR2d(4 * nker, 2 * nker, kernel_size=4, stride=2,\n",
        "                            padding=1, norm=norm, relu=0.0, bias=False)\n",
        "\n",
        "        self.dec4 = DECBR2d(2 * nker, 1 * nker, kernel_size=4, stride=2,\n",
        "                            padding=1, norm=norm, relu=0.0, bias=False)\n",
        "\n",
        "        self.dec5 = DECBR2d(1 * nker, out_channels, kernel_size=4, stride=2,\n",
        "                            padding=1, norm=None, relu=None, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.dec1(x)\n",
        "        x = self.dec2(x)\n",
        "        x = self.dec3(x)\n",
        "        x = self.dec4(x)\n",
        "        x = self.dec5(x)\n",
        "\n",
        "        x = torch.tanh(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "ZGysNUxNd6OM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Discriminator**"
      ],
      "metadata": {
        "id": "NlojCFC_d9xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, nker=64, norm=\"bnorm\"):\n",
        "        super(Discriminator, self).__init__()\n",
        "\n",
        "        self.enc1 = CBR2d(1 * in_channels, 1 * nker, kernel_size=4, stride=2,\n",
        "                          padding=1, norm=None, relu=0.2, bias=False)\n",
        "\n",
        "        self.enc2 = CBR2d(1 * nker, 2 * nker, kernel_size=4, stride=2,\n",
        "                          padding=1, norm=norm, relu=0.2, bias=False)\n",
        "\n",
        "        self.enc3 = CBR2d(2 * nker, 4 * nker, kernel_size=4, stride=2,\n",
        "                          padding=1, norm=norm, relu=0.2, bias=False)\n",
        "\n",
        "        self.enc4 = CBR2d(4 * nker, 8 * nker, kernel_size=4, stride=2,\n",
        "                          padding=1, norm=norm, relu=0.2, bias=False)\n",
        "\n",
        "        self.enc5 = CBR2d(8 * nker, out_channels, kernel_size=4, stride=2,\n",
        "                          padding=1, norm=None, relu=None, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.enc1(x)\n",
        "        x = self.enc2(x)\n",
        "        x = self.enc3(x)\n",
        "        x = self.enc4(x)\n",
        "        x = self.enc5(x)\n",
        "\n",
        "        x = torch.sigmoid(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "mGtoBywmeAYJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.U-Net: Convolutional Networks for Biomedical Image Segmentation**"
      ],
      "metadata": {
        "id": "Jn6T9_j6eC4q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/1505.04597"
      ],
      "metadata": {
        "id": "LYJOrccleGkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, nker=64, learning_type=\"plain\", norm=\"bnorm\"):\n",
        "        super(UNet, self).__init__()\n",
        "\n",
        "        self.learning_type = learning_type\n",
        "\n",
        "        # Contracting path\n",
        "        self.enc1_1 = CBR2d(in_channels=in_channels, out_channels=1 * nker, norm=norm)\n",
        "        self.enc1_2 = CBR2d(in_channels=1 * nker, out_channels=1 * nker, norm=norm)\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc2_1 = CBR2d(in_channels=nker, out_channels=2 * nker, norm=norm)\n",
        "        self.enc2_2 = CBR2d(in_channels=2 * nker, out_channels=2 * nker, norm=norm)\n",
        "\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc3_1 = CBR2d(in_channels=2 * nker, out_channels=4 * nker, norm=norm)\n",
        "        self.enc3_2 = CBR2d(in_channels=4 * nker, out_channels=4 * nker, norm=norm)\n",
        "\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc4_1 = CBR2d(in_channels=4 * nker, out_channels=8 * nker, norm=norm)\n",
        "        self.enc4_2 = CBR2d(in_channels=8 * nker, out_channels=8 * nker, norm=norm)\n",
        "\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc5_1 = CBR2d(in_channels=8 * nker, out_channels=16 * nker, norm=norm)\n",
        "\n",
        "        # Expansive path\n",
        "        self.dec5_1 = CBR2d(in_channels=16 * nker, out_channels=8 * nker, norm=norm)\n",
        "\n",
        "        self.unpool4 = nn.ConvTranspose2d(in_channels=8 * nker, out_channels=8 * nker,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec4_2 = CBR2d(in_channels=2 * 8 * nker, out_channels=8 * nker, norm=norm)\n",
        "        self.dec4_1 = CBR2d(in_channels=8 * nker, out_channels=4 * nker, norm=norm)\n",
        "\n",
        "        self.unpool3 = nn.ConvTranspose2d(in_channels=4 * nker, out_channels=4 * nker,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec3_2 = CBR2d(in_channels=2 * 4 * nker, out_channels=4 * nker, norm=norm)\n",
        "        self.dec3_1 = CBR2d(in_channels=4 * nker, out_channels=2 * nker, norm=norm)\n",
        "\n",
        "        self.unpool2 = nn.ConvTranspose2d(in_channels=2 * nker, out_channels=2 * nker,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec2_2 = CBR2d(in_channels=2 * 2 * nker, out_channels=2 * nker, norm=norm)\n",
        "        self.dec2_1 = CBR2d(in_channels=2 * nker, out_channels=1 * nker, norm=norm)\n",
        "\n",
        "        self.unpool1 = nn.ConvTranspose2d(in_channels=1 * nker, out_channels=1 * nker,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec1_2 = CBR2d(in_channels=2 * 1 * nker, out_channels=1 * nker, norm=norm)\n",
        "        self.dec1_1 = CBR2d(in_channels=1 * nker, out_channels=1 * nker, norm=norm)\n",
        "\n",
        "        self.fc = nn.Conv2d(in_channels=1 * nker, out_channels=out_channels, kernel_size=1, stride=1, padding=0, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1_1 = self.enc1_1(x)\n",
        "        enc1_2 = self.enc1_2(enc1_1)\n",
        "        pool1 = self.pool1(enc1_2)\n",
        "\n",
        "        enc2_1 = self.enc2_1(pool1)\n",
        "        enc2_2 = self.enc2_2(enc2_1)\n",
        "        pool2 = self.pool2(enc2_2)\n",
        "\n",
        "        enc3_1 = self.enc3_1(pool2)\n",
        "        enc3_2 = self.enc3_2(enc3_1)\n",
        "        pool3 = self.pool3(enc3_2)\n",
        "\n",
        "        enc4_1 = self.enc4_1(pool3)\n",
        "        enc4_2 = self.enc4_2(enc4_1)\n",
        "        pool4 = self.pool4(enc4_2)\n",
        "\n",
        "        enc5_1 = self.enc5_1(pool4)\n",
        "\n",
        "        dec5_1 = self.dec5_1(enc5_1)\n",
        "\n",
        "        unpool4 = self.unpool4(dec5_1)\n",
        "        cat4 = torch.cat((unpool4, enc4_2), dim=1)\n",
        "        dec4_2 = self.dec4_2(cat4)\n",
        "        dec4_1 = self.dec4_1(dec4_2)\n",
        "\n",
        "        unpool3 = self.unpool3(dec4_1)\n",
        "        cat3 = torch.cat((unpool3, enc3_2), dim=1)\n",
        "        dec3_2 = self.dec3_2(cat3)\n",
        "        dec3_1 = self.dec3_1(dec3_2)\n",
        "\n",
        "        unpool2 = self.unpool2(dec3_1)\n",
        "        cat2 = torch.cat((unpool2, enc2_2), dim=1)\n",
        "        dec2_2 = self.dec2_2(cat2)\n",
        "        dec2_1 = self.dec2_1(dec2_2)\n",
        "\n",
        "        unpool1 = self.unpool1(dec2_1)\n",
        "        cat1 = torch.cat((unpool1, enc1_2), dim=1)\n",
        "        dec1_2 = self.dec1_2(cat1)\n",
        "        dec1_1 = self.dec1_1(dec1_2)\n",
        "\n",
        "        if self.learning_type == \"plain\":\n",
        "            x = self.fc(dec1_1)\n",
        "        elif self.learning_type == \"residual\":\n",
        "            x = x + self.fc(dec1_1)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "asi6VW-BeKVq"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hourglass**"
      ],
      "metadata": {
        "id": "TNMhd9XueTWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Hourglass(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, nker=64, learning_type=\"plain\", norm=\"bnorm\"):\n",
        "        super(Hourglass, self).__init__()\n",
        "\n",
        "        self.learning_type = learning_type\n",
        "\n",
        "        # Contracting path\n",
        "        self.enc1_1 = CBR2d(in_channels=in_channels, out_channels=1 * nker, norm=norm)\n",
        "        self.enc1_2 = CBR2d(in_channels=1 * nker, out_channels=1 * nker, norm=norm)\n",
        "\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc2_1 = CBR2d(in_channels=1 * nker, out_channels=2 * nker, norm=norm)\n",
        "        self.enc2_2 = CBR2d(in_channels=2 * nker, out_channels=2 * nker, norm=norm)\n",
        "\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc3_1 = CBR2d(in_channels=2 * nker, out_channels=4 * nker, norm=norm)\n",
        "        self.enc3_2 = CBR2d(in_channels=4 * nker, out_channels=4 * nker, norm=norm)\n",
        "\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc4_1 = CBR2d(in_channels=4 * nker, out_channels=8 * nker, norm=norm)\n",
        "        self.enc4_2 = CBR2d(in_channels=8 * nker, out_channels=8 * nker, norm=norm)\n",
        "\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "        self.enc5_1 = CBR2d(in_channels=8 * nker, out_channels=16 * nker, norm=norm)\n",
        "\n",
        "        # Expansive path\n",
        "        self.dec5_1 = CBR2d(in_channels=16 * nker, out_channels=8 * nker, norm=norm)\n",
        "\n",
        "        self.unpool4 = nn.ConvTranspose2d(in_channels=8 * nker, out_channels=8 * nker,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec4_2 = CBR2d(in_channels=1 * 8 * nker, out_channels=8 * nker, norm=norm)\n",
        "        self.dec4_1 = CBR2d(in_channels=8 * nker, out_channels=4 * nker, norm=norm)\n",
        "\n",
        "        self.unpool3 = nn.ConvTranspose2d(in_channels=4 * nker, out_channels=4 * nker,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec3_2 = CBR2d(in_channels=1 * 4 * nker, out_channels=4 * nker, norm=norm)\n",
        "        self.dec3_1 = CBR2d(in_channels=4 * nker, out_channels=2 * nker, norm=norm)\n",
        "\n",
        "        self.unpool2 = nn.ConvTranspose2d(in_channels=2 * nker, out_channels=2 * nker,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec2_2 = CBR2d(in_channels=1 * 2 * nker, out_channels=2 * nker, norm=norm)\n",
        "        self.dec2_1 = CBR2d(in_channels=2 * nker, out_channels=1 * nker, norm=norm)\n",
        "\n",
        "        self.unpool1 = nn.ConvTranspose2d(in_channels=1 * nker, out_channels=1 * nker,\n",
        "                                          kernel_size=2, stride=2, padding=0, bias=True)\n",
        "\n",
        "        self.dec1_2 = CBR2d(in_channels=1 * 1 * nker, out_channels=1 * nker, norm=norm)\n",
        "        self.dec1_1 = CBR2d(in_channels=1 * nker, out_channels=1 * nker, norm=norm)\n",
        "\n",
        "        self.fc = CBR2d(in_channels=1 * nker, out_channels=out_channels, kernel_size=1, stride=1, padding=0, bias=True, norm=None, relu=None)\n",
        "\n",
        "    def forward(self, x):\n",
        "        enc1_1 = self.enc1_1(x)\n",
        "        enc1_2 = self.enc1_2(enc1_1)\n",
        "        pool1 = self.pool1(enc1_2)\n",
        "\n",
        "        enc2_1 = self.enc2_1(pool1)\n",
        "        enc2_2 = self.enc2_2(enc2_1)\n",
        "        pool2 = self.pool2(enc2_2)\n",
        "\n",
        "        enc3_1 = self.enc3_1(pool2)\n",
        "        enc3_2 = self.enc3_2(enc3_1)\n",
        "        pool3 = self.pool3(enc3_2)\n",
        "\n",
        "        enc4_1 = self.enc4_1(pool3)\n",
        "        enc4_2 = self.enc4_2(enc4_1)\n",
        "        pool4 = self.pool4(enc4_2)\n",
        "\n",
        "        enc5_1 = self.enc5_1(pool4)\n",
        "\n",
        "        dec5_1 = self.dec5_1(enc5_1)\n",
        "\n",
        "        unpool4 = self.unpool4(dec5_1)\n",
        "        # cat4 = torch.cat((unpool4, enc4_2), dim=1)\n",
        "        cat4 = unpool4\n",
        "        dec4_2 = self.dec4_2(cat4)\n",
        "        dec4_1 = self.dec4_1(dec4_2)\n",
        "\n",
        "        unpool3 = self.unpool3(dec4_1)\n",
        "        # cat3 = torch.cat((unpool3, enc3_2), dim=1)\n",
        "        cat3 = unpool3\n",
        "        dec3_2 = self.dec3_2(cat3)\n",
        "        dec3_1 = self.dec3_1(dec3_2)\n",
        "\n",
        "        unpool2 = self.unpool2(dec3_1)\n",
        "        # cat2 = torch.cat((unpool2, enc2_2), dim=1)\n",
        "        cat2 = unpool2\n",
        "        dec2_2 = self.dec2_2(cat2)\n",
        "        dec2_1 = self.dec2_1(dec2_2)\n",
        "\n",
        "        unpool1 = self.unpool1(dec2_1)\n",
        "        # cat1 = torch.cat((unpool1, enc1_2), dim=1)\n",
        "        cat1 = unpool1\n",
        "        dec1_2 = self.dec1_2(cat1)\n",
        "        dec1_1 = self.dec1_1(dec1_2)\n",
        "\n",
        "        if self.learning_type == \"plain\":\n",
        "            x = self.fc(dec1_1)\n",
        "        elif self.learning_type == \"residual\":\n",
        "            x = x + self.fc(dec1_1)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "gZLfklubefIp"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Resnet**"
      ],
      "metadata": {
        "id": "DJU4sx4xehhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/1512.03385"
      ],
      "metadata": {
        "id": "WxJSd7AXekqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, nker=64, learning_type=\"plain\", norm=\"bnorm\", nblk=16):\n",
        "        super(ResNet, self).__init__()\n",
        "\n",
        "        self.learning_type = learning_type\n",
        "\n",
        "        self.enc = CBR2d(in_channels, nker, kernel_size=3, stride=1, padding=1, bias=True, norm=None, relu=0.0)\n",
        "\n",
        "        res = []\n",
        "        for i in range(nblk):\n",
        "            res += [ResBlock(nker, nker, kernel_size=3, stride=1, padding=1, bias=True, norm=norm, relu=0.0)]\n",
        "        self.res = nn.Sequential(*res)\n",
        "\n",
        "        self.dec = CBR2d(nker, nker, kernel_size=3, stride=1, padding=1, bias=True, norm=norm, relu=0.0)\n",
        "\n",
        "        self.fc = CBR2d(nker, out_channels, kernel_size=1, stride=1, padding=0, bias=True, norm=None, relu=None)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x0 = x\n",
        "\n",
        "        x = self.enc(x)\n",
        "        x = self.res(x)\n",
        "        x = self.dec(x)\n",
        "\n",
        "        if self.learning_type == \"plain\":\n",
        "            x = self.fc(x)\n",
        "        elif self.learning_type == \"residual\":\n",
        "            x = x0 + self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "jdeRk62leofj"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SRResNet**"
      ],
      "metadata": {
        "id": "AnTTflpResTZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://arxiv.org/abs/1609.04802"
      ],
      "metadata": {
        "id": "wa1bYjRFeygy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Photo-Realistic Single Image Super-Resolution Using a Generative Adversarial Network\n",
        "class SRResNet(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, nker=64, learning_type=\"plain\", norm=\"bnorm\", nblk=16):\n",
        "        super(SRResNet, self).__init__()\n",
        "\n",
        "        self.learning_type = learning_type\n",
        "\n",
        "        self.enc = CBR2d(in_channels, nker, kernel_size=9, stride=1, padding=4, bias=True, norm=None, relu=0.0)\n",
        "\n",
        "        res = []\n",
        "        for i in range(nblk):\n",
        "            res += [ResBlock(nker, nker, kernel_size=3, stride=1, padding=1, bias=True, norm=norm, relu=0.0)]\n",
        "        self.res = nn.Sequential(*res)\n",
        "        self.dec = CBR2d(nker, nker, kernel_size=3, stride=1, padding=1, bias=True, norm=norm, relu=None)\n",
        "\n",
        "        # ps1 = []\n",
        "        # ps1 += [nn.Conv2d(in_channels=nker, out_channels=nker, kernel_size=3, stride=1, padding=1)]\n",
        "        # ps1 += [nn.ReLU()]\n",
        "        # self.ps1 = nn.Sequential(*ps1)\n",
        "        #\n",
        "        # ps2 = []\n",
        "        # ps2 += [nn.Conv2d(in_channels=nker, out_channels=nker, kernel_size=3, stride=1, padding=1)]\n",
        "        # ps2 += [nn.ReLU()]\n",
        "        # self.ps2 = nn.Sequential(*ps2)\n",
        "\n",
        "        ps1 = []\n",
        "        ps1 += [nn.Conv2d(in_channels=nker, out_channels=4 * nker, kernel_size=3, stride=1, padding=1)]\n",
        "        ps1 += [PixelShuffle(ry=2, rx=2)]\n",
        "        ps1 += [nn.ReLU()]\n",
        "        self.ps1 = nn.Sequential(*ps1)\n",
        "\n",
        "        ps2 = []\n",
        "        ps2 += [nn.Conv2d(in_channels=nker, out_channels=4 * nker, kernel_size=3, stride=1, padding=1)]\n",
        "        ps2 += [PixelShuffle(ry=2, rx=2)]\n",
        "        ps2 += [nn.ReLU()]\n",
        "        self.ps2 = nn.Sequential(*ps2)\n",
        "\n",
        "        self.fc = CBR2d(nker, out_channels, kernel_size=9, stride=1, padding=4, bias=True, norm=None, relu=None)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.enc(x)\n",
        "        x0 = x\n",
        "\n",
        "        x = self.res(x)\n",
        "\n",
        "        x = self.dec(x)\n",
        "        x = x + x0\n",
        "\n",
        "        x = self.ps1(x)\n",
        "        x = self.ps2(x)\n",
        "\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "fDbTtfUhdV8K"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training**"
      ],
      "metadata": {
        "id": "C0X13LOxaxON"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MEAN = 0.5\n",
        "STD = 0.5\n",
        "\n",
        "NUM_WORKER = 0"
      ],
      "metadata": {
        "id": "k_kAHLvEa8eY"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train Function**"
      ],
      "metadata": {
        "id": "IyY89KzGbJzd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args):\n",
        "    mode = args.mode\n",
        "    train_continue = args.train_continue\n",
        "\n",
        "    lr = args.lr\n",
        "    batch_size = args.batch_size\n",
        "    num_epoch = args.num_epoch\n",
        "\n",
        "    data_dir = args.data_dir\n",
        "    ckpt_dir = args.ckpt_dir\n",
        "    log_dir = args.log_dir\n",
        "    result_dir = args.result_dir\n",
        "\n",
        "    task = args.task\n",
        "    opts = [args.opts[0], np.asarray(args.opts[1:]).astype(np.float)]\n",
        "\n",
        "    ny = args.ny\n",
        "    nx = args.nx\n",
        "    nch = args.nch\n",
        "    nker = args.nker\n",
        "\n",
        "    wgt_cycle = args.wgt_cycle\n",
        "    wgt_ident = args.wgt_ident\n",
        "    norm = args.norm\n",
        "\n",
        "    network = args.network\n",
        "    learning_type = args.learning_type\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(\"mode: %s\" % mode)\n",
        "    print(\"norm: %s\" % norm)\n",
        "\n",
        "    print(\"learning rate: %.4e\" % lr)\n",
        "    print(\"batch size: %d\" % batch_size)\n",
        "    print(\"number of epoch: %d\" % num_epoch)\n",
        "\n",
        "    print(\"task: %s\" % task)\n",
        "    print(\"opts: %s\" % opts)\n",
        "\n",
        "    print(\"network: %s\" % network)\n",
        "    print(\"learning type: %s\" % learning_type)\n",
        "\n",
        "    print(\"data dir: %s\" % data_dir)\n",
        "    print(\"ckpt dir: %s\" % ckpt_dir)\n",
        "    print(\"log dir: %s\" % log_dir)\n",
        "    print(\"result dir: %s\" % result_dir)\n",
        "\n",
        "    print(\"device: %s\" % device)\n",
        "\n",
        "    result_dir_train = os.path.join(result_dir, 'train')\n",
        "\n",
        "    if not os.path.exists(result_dir_train):\n",
        "        os.makedirs(os.path.join(result_dir_train, 'png', 'a2b'))\n",
        "        os.makedirs(os.path.join(result_dir_train, 'png', 'b2a'))\n",
        "\n",
        "    if mode == 'train':\n",
        "        transform_train = transforms.Compose([Resize(shape=(286, 286, nch)),\n",
        "                                              RandomCrop((ny, nx)),\n",
        "                                              Normalization(mean=MEAN, std=STD)])\n",
        "\n",
        "        dataset_train = Dataset(data_dir=os.path.join(data_dir, 'train'),\n",
        "                                transform=transform_train,\n",
        "                                task=task, data_type='both')\n",
        "        loader_train = DataLoader(dataset_train, batch_size=batch_size,\n",
        "                                  shuffle=True, num_workers=NUM_WORKER)\n",
        "\n",
        "        num_data_train = len(dataset_train)\n",
        "        num_batch_train = np.ceil(num_data_train / batch_size)\n",
        "\n",
        "    if network == \"CycleGAN\":\n",
        "        netG_a2b = CycleGAN(in_channels=nch, out_channels=nch, nker=nker, norm=norm, nblk=9).to(device)\n",
        "        netG_b2a = CycleGAN(in_channels=nch, out_channels=nch, nker=nker, norm=norm, nblk=9).to(device)\n",
        "\n",
        "        netD_a = Discriminator(in_channels=nch, out_channels=1, nker=nker, norm=norm).to(device)\n",
        "        netD_b = Discriminator(in_channels=nch, out_channels=1, nker=nker, norm=norm).to(device)\n",
        "\n",
        "        init_weights(netG_a2b, init_type='normal', init_gain=0.02)\n",
        "        init_weights(netG_b2a, init_type='normal', init_gain=0.02)\n",
        "\n",
        "        init_weights(netD_a, init_type='normal', init_gain=0.02)\n",
        "        init_weights(netD_b, init_type='normal', init_gain=0.02)\n",
        "\n",
        "    fn_cycle = nn.L1Loss().to(device)\n",
        "    fn_gan = nn.BCELoss().to(device)\n",
        "    fn_ident = nn.L1Loss().to(device)\n",
        "\n",
        "    optimG = torch.optim.Adam(itertools.chain(netG_a2b.parameters(), netG_b2a.parameters()), lr=lr, betas=(0.5, 0.999))\n",
        "    optimD = torch.optim.Adam(itertools.chain(netD_a.parameters(), netD_b.parameters()), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    fn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\n",
        "    fn_denorm = lambda x: (x * STD) + MEAN\n",
        "\n",
        "    cmap = None\n",
        "\n",
        "    writer_train = SummaryWriter(log_dir=os.path.join(log_dir, 'train'))\n",
        "\n",
        "    st_epoch = 0\n",
        "\n",
        "    # TRAIN MODE\n",
        "    if mode == 'train':\n",
        "        if train_continue == \"on\":\n",
        "            netG_a2b, netG_b2a, \\\n",
        "            netD_a, netD_b, \\\n",
        "            optimG, optimD, st_epoch = load(ckpt_dir=ckpt_dir,\n",
        "                                            netG_a2b=netG_a2b, netG_b2a=netG_b2a,\n",
        "                                            netD_a=netD_a, netD_b=netD_b,\n",
        "                                            optimG=optimG, optimD=optimD)\n",
        "\n",
        "        for epoch in range(st_epoch + 1, num_epoch + 1):\n",
        "            netG_a2b.train()\n",
        "            netG_b2a.train()\n",
        "            netD_a.train()\n",
        "            netD_b.train()\n",
        "\n",
        "            loss_G_a2b_train = []\n",
        "            loss_G_b2a_train = []\n",
        "            loss_D_a_train = []\n",
        "            loss_D_b_train = []\n",
        "            loss_cycle_a_train = []\n",
        "            loss_cycle_b_train = []\n",
        "            loss_ident_a_train = []\n",
        "            loss_ident_b_train = []\n",
        "\n",
        "            for batch, data in enumerate(loader_train, 1):\n",
        "                input_a = data['data_a'].to(device)\n",
        "                input_b = data['data_b'].to(device)\n",
        "\n",
        "                # forward netG\n",
        "                output_b = netG_a2b(input_a)\n",
        "                output_a = netG_b2a(input_b)\n",
        "\n",
        "                recon_b = netG_a2b(output_a)\n",
        "                recon_a = netG_b2a(output_b)\n",
        "\n",
        "                # backward netD\n",
        "                set_requires_grad([netD_a, netD_b], True)\n",
        "                optimD.zero_grad()\n",
        "\n",
        "                # backward netD_a\n",
        "                pred_real_a = netD_a(input_a)\n",
        "                pred_fake_a = netD_a(output_a.detach())\n",
        "\n",
        "                loss_D_a_real = fn_gan(pred_real_a, torch.ones_like(pred_real_a))\n",
        "                loss_D_a_fake = fn_gan(pred_fake_a, torch.zeros_like(pred_fake_a))\n",
        "                loss_D_a = 0.5 * (loss_D_a_real + loss_D_a_fake)\n",
        "\n",
        "                # backward netD_b\n",
        "                pred_real_b = netD_b(input_b)\n",
        "                pred_fake_b = netD_b(output_b.detach())\n",
        "\n",
        "                loss_D_b_real = fn_gan(pred_real_b, torch.ones_like(pred_real_b))\n",
        "                loss_D_b_fake = fn_gan(pred_fake_b, torch.zeros_like(pred_fake_b))\n",
        "                loss_D_b = 0.5 * (loss_D_b_real + loss_D_b_fake)\n",
        "\n",
        "                loss_D = loss_D_a + loss_D_b\n",
        "                loss_D.backward()\n",
        "                optimD.step()\n",
        "\n",
        "                # backward netG\n",
        "                set_requires_grad([netD_a, netD_b], False)\n",
        "                optimG.zero_grad()\n",
        "\n",
        "                pred_fake_a = netD_a(output_a)\n",
        "                pred_fake_b = netD_b(output_b)\n",
        "\n",
        "                loss_G_a2b = fn_gan(pred_fake_a, torch.ones_like(pred_fake_a))\n",
        "                loss_G_b2a = fn_gan(pred_fake_b, torch.ones_like(pred_fake_b))\n",
        "\n",
        "                loss_cycle_a = fn_cycle(input_a, recon_a)\n",
        "                loss_cycle_b = fn_cycle(input_b, recon_b)\n",
        "\n",
        "                ident_a = netG_b2a(input_a)\n",
        "                ident_b = netG_a2b(input_b)\n",
        "\n",
        "                loss_ident_a = fn_ident(input_a, ident_a)\n",
        "                loss_ident_b = fn_ident(input_b, ident_b)\n",
        "\n",
        "                loss_G = (loss_G_a2b + loss_G_b2a) + \\\n",
        "                         wgt_cycle * (loss_cycle_a + loss_cycle_b) + \\\n",
        "                         wgt_cycle * wgt_ident * (loss_ident_a + loss_ident_b)\n",
        "\n",
        "                loss_G.backward()\n",
        "                optimG.step()\n",
        "\n",
        "                #\n",
        "                loss_G_a2b_train += [loss_G_a2b.item()]\n",
        "                loss_G_b2a_train += [loss_G_b2a.item()]\n",
        "\n",
        "                loss_D_a_train += [loss_D_a.item()]\n",
        "                loss_D_b_train += [loss_D_b.item()]\n",
        "\n",
        "                loss_cycle_a_train += [loss_cycle_a.item()]\n",
        "                loss_cycle_b_train += [loss_cycle_b.item()]\n",
        "\n",
        "                loss_ident_a_train += [loss_ident_a.item()]\n",
        "                loss_ident_b_train += [loss_ident_b.item()]\n",
        "\n",
        "                print(\"TRAIN: EPOCH %04d / %04d | BATCH %04d / %04d | \"\n",
        "                      \"GEN a2b %.4f b2a %.4f | \"\n",
        "                      \"DISC a %.4f b %.4f | \"\n",
        "                      \"CYCLE a %.4f b %.4f | \"\n",
        "                      \"IDENT a %.4f b %.4f | \" %\n",
        "                      (epoch, num_epoch, batch, num_batch_train,\n",
        "                       np.mean(loss_G_a2b_train), np.mean(loss_G_b2a_train),\n",
        "                       np.mean(loss_D_a_train), np.mean(loss_D_b_train),\n",
        "                       np.mean(loss_cycle_a_train), np.mean(loss_cycle_b_train),\n",
        "                       np.mean(loss_ident_a_train), np.mean(loss_ident_b_train)))\n",
        "\n",
        "                if batch % 20 == 0:\n",
        "                    # Tensorboard\n",
        "                    input_a = fn_tonumpy(fn_denorm(input_a)).squeeze()\n",
        "                    input_b = fn_tonumpy(fn_denorm(input_b)).squeeze()\n",
        "                    output_a = fn_tonumpy(fn_denorm(output_a)).squeeze()\n",
        "                    output_b = fn_tonumpy(fn_denorm(output_b)).squeeze()\n",
        "\n",
        "                    input_a = np.clip(input_a, a_min=0, a_max=1)\n",
        "                    input_b = np.clip(input_b, a_min=0, a_max=1)\n",
        "                    output_a = np.clip(output_a, a_min=0, a_max=1)\n",
        "                    output_b = np.clip(output_b, a_min=0, a_max=1)\n",
        "\n",
        "                    id = num_batch_train * (epoch - 1) + batch\n",
        "\n",
        "                    plt.imsave(os.path.join(result_dir_train, 'png', 'a2b', '%04d_input_a.png' % id), input_a[0],\n",
        "                               cmap=cmap)\n",
        "                    plt.imsave(os.path.join(result_dir_train, 'png', 'a2b', '%04d_output_b.png' % id), output_b[0],\n",
        "                               cmap=cmap)\n",
        "\n",
        "                    plt.imsave(os.path.join(result_dir_train, 'png', 'b2a', '%04d_input_b.png' % id), input_b[0],\n",
        "                               cmap=cmap)\n",
        "                    plt.imsave(os.path.join(result_dir_train, 'png', 'b2a', '%04d_output_a.png' % id), output_a[0],\n",
        "                               cmap=cmap)\n",
        "\n",
        "                    writer_train.add_image('input_a', input_a, id, dataformats='NHWC')\n",
        "                    writer_train.add_image('input_b', input_b, id, dataformats='NHWC')\n",
        "                    writer_train.add_image('output_a', output_a, id, dataformats='NHWC')\n",
        "                    writer_train.add_image('output_b', output_b, id, dataformats='NHWC')\n",
        "\n",
        "            writer_train.add_scalar('loss_G_a2b', np.mean(loss_G_a2b_train), epoch)\n",
        "            writer_train.add_scalar('loss_G_b2a', np.mean(loss_G_b2a_train), epoch)\n",
        "\n",
        "            writer_train.add_scalar('loss_D_a', np.mean(loss_D_a_train), epoch)\n",
        "            writer_train.add_scalar('loss_D_b', np.mean(loss_D_b_train), epoch)\n",
        "\n",
        "            writer_train.add_scalar('loss_cycle_a', np.mean(loss_cycle_a_train), epoch)\n",
        "            writer_train.add_scalar('loss_cycle_b', np.mean(loss_cycle_b_train), epoch)\n",
        "\n",
        "            writer_train.add_scalar('loss_ident_a', np.mean(loss_ident_a_train), epoch)\n",
        "            writer_train.add_scalar('loss_ident_b', np.mean(loss_ident_b_train), epoch)\n",
        "\n",
        "            if epoch % 2 == 0 or epoch == num_epoch:\n",
        "                save(ckpt_dir=ckpt_dir, epoch=epoch,\n",
        "                     netG_a2b=netG_a2b, netG_b2a=netG_b2a,\n",
        "                     netD_a=netD_a, netD_b=netD_b,\n",
        "                     optimG=optimG, optimD=optimD)\n",
        "\n",
        "        writer_train.close()\n"
      ],
      "metadata": {
        "id": "_0nMSEfrbIB1"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(args):\n",
        "    mode = args.mode\n",
        "    train_continue = args.train_continue\n",
        "\n",
        "    lr = args.lr\n",
        "    batch_size = args.batch_size\n",
        "    num_epoch = args.num_epoch\n",
        "\n",
        "    data_dir = args.data_dir\n",
        "    ckpt_dir = args.ckpt_dir\n",
        "    log_dir = args.log_dir\n",
        "    result_dir = args.result_dir\n",
        "\n",
        "    task = args.task\n",
        "    opts = [args.opts[0], np.asarray(args.opts[1:]).astype(np.float)]\n",
        "\n",
        "    ny = args.ny\n",
        "    nx = args.nx\n",
        "    nch = args.nch\n",
        "    nker = args.nker\n",
        "\n",
        "    wgt_cycle = args.wgt_cycle\n",
        "    wgt_ident = args.wgt_ident\n",
        "    norm = args.norm\n",
        "\n",
        "    network = args.network\n",
        "    learning_type = args.learning_type\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    print(\"mode: %s\" % mode)\n",
        "\n",
        "    print(\"learning rate: %.4e\" % lr)\n",
        "    print(\"batch size: %d\" % batch_size)\n",
        "    print(\"number of epoch: %d\" % num_epoch)\n",
        "\n",
        "    print(\"task: %s\" % task)\n",
        "    print(\"opts: %s\" % opts)\n",
        "\n",
        "    print(\"network: %s\" % network)\n",
        "    print(\"learning type: %s\" % learning_type)\n",
        "\n",
        "    print(\"data dir: %s\" % data_dir)\n",
        "    print(\"ckpt dir: %s\" % ckpt_dir)\n",
        "    print(\"log dir: %s\" % log_dir)\n",
        "    print(\"result dir: %s\" % result_dir)\n",
        "\n",
        "    print(\"device: %s\" % device)\n",
        "\n",
        "    ##\n",
        "    result_dir_test = os.path.join(result_dir, 'test')\n",
        "\n",
        "    if not os.path.exists(result_dir_test):\n",
        "        os.makedirs(os.path.join(result_dir_test, 'png', 'a2b'))\n",
        "        os.makedirs(os.path.join(result_dir_test, 'png', 'b2a'))\n",
        "        # os.makedirs(os.path.join(result_dir_test, 'numpy'))\n",
        "\n",
        "    ##\n",
        "    if mode == 'test':\n",
        "        transform_test = transforms.Compose([Resize(shape=(ny, nx, nch)), Normalization(mean=MEAN, std=STD)])\n",
        "\n",
        "        dataset_test_a = Dataset(data_dir=os.path.join(data_dir, 'test'), transform=transform_test, task=task,\n",
        "                                 data_type='a')\n",
        "        loader_test_a = DataLoader(dataset_test_a, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKER)\n",
        "\n",
        "        #\n",
        "        num_data_test_a = len(dataset_test_a)\n",
        "        num_batch_test_a = np.ceil(num_data_test_a / batch_size)\n",
        "\n",
        "        dataset_test_b = Dataset(data_dir=os.path.join(data_dir, 'test'), transform=transform_test, task=task,\n",
        "                                 data_type='b')\n",
        "        loader_test_b = DataLoader(dataset_test_b, batch_size=batch_size, shuffle=False, num_workers=NUM_WORKER)\n",
        "\n",
        "        #\n",
        "        num_data_test_b = len(dataset_test_b)\n",
        "        num_batch_test_b = np.ceil(num_data_test_b / batch_size)\n",
        "\n",
        "    ##\n",
        "    if network == \"CycleGAN\":\n",
        "        netG_a2b = CycleGAN(in_channels=nch, out_channels=nch, nker=nker, norm=norm, nblk=9).to(device)\n",
        "        netG_b2a = CycleGAN(in_channels=nch, out_channels=nch, nker=nker, norm=norm, nblk=9).to(device)\n",
        "\n",
        "        netD_a = Discriminator(in_channels=nch, out_channels=1, nker=nker, norm=norm).to(device)\n",
        "        netD_b = Discriminator(in_channels=nch, out_channels=1, nker=nker, norm=norm).to(device)\n",
        "\n",
        "        init_weights(netG_a2b, init_type='normal', init_gain=0.02)\n",
        "        init_weights(netG_b2a, init_type='normal', init_gain=0.02)\n",
        "\n",
        "        init_weights(netD_a, init_type='normal', init_gain=0.02)\n",
        "        init_weights(netD_b, init_type='normal', init_gain=0.02)\n",
        "\n",
        "    ##\n",
        "    fn_cycle = nn.L1Loss().to(device)\n",
        "    fn_gan = nn.BCELoss().to(device)\n",
        "    fn_ident = nn.L1Loss().to(device)\n",
        "\n",
        "    ## Optimizer\n",
        "    optimG = torch.optim.Adam(itertools.chain(netG_a2b.parameters(), netG_b2a.parameters()), lr=lr, betas=(0.5, 0.999))\n",
        "    optimD = torch.optim.Adam(itertools.chain(netD_a.parameters(), netD_b.parameters()), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    ##\n",
        "    fn_tonumpy = lambda x: x.to('cpu').detach().numpy().transpose(0, 2, 3, 1)\n",
        "    fn_denorm = lambda x: (x * STD) + MEAN\n",
        "\n",
        "    ##\n",
        "    st_epoch = 0\n",
        "\n",
        "    # TRAIN MODE\n",
        "    if mode == \"test\":\n",
        "        netG_a2b, netG_b2a, \\\n",
        "        netD_a, netD_b, \\\n",
        "        optimG, optimD, st_epoch = load(ckpt_dir=ckpt_dir,\n",
        "                                        netG_a2b=netG_a2b, netG_b2a=netG_b2a,\n",
        "                                        netD_a=netD_a, netD_b=netD_b,\n",
        "                                        optimG=optimG, optimD=optimD)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            netG_a2b.eval()\n",
        "            netG_b2a.eval()\n",
        "\n",
        "            for batch, data in enumerate(loader_test_a, 1):\n",
        "                # forward pass\n",
        "                input_a = data['data_a'].to(device)\n",
        "\n",
        "                output_b = netG_a2b(input_a)\n",
        "\n",
        "                # Tensorboard\n",
        "                input_a = fn_tonumpy(fn_denorm(input_a))\n",
        "                output_b = fn_tonumpy(fn_denorm(output_b))\n",
        "\n",
        "                for j in range(input_a.shape[0]):\n",
        "                    id = batch_size * (batch - 1) + j\n",
        "\n",
        "                    input_a_ = input_a[j]\n",
        "                    output_b_ = output_b[j]\n",
        "\n",
        "                    input_a_ = np.clip(input_a_, a_min=0, a_max=1)\n",
        "                    output_b_ = np.clip(output_b_, a_min=0, a_max=1)\n",
        "\n",
        "                    plt.imsave(os.path.join(result_dir_test, 'png', 'a2b', '%04d_input_a.png' % id), input_a_)\n",
        "                    plt.imsave(os.path.join(result_dir_test, 'png', 'a2b', '%04d_output_b.png' % id), output_b_)\n",
        "\n",
        "                    print(\"TEST A: BATCH %04d / %04d | \" % (id + 1, num_data_test_a))\n",
        "\n",
        "            for batch, data in enumerate(loader_test_b, 1):\n",
        "                # forward pass\n",
        "                input_b = data['data_b'].to(device)\n",
        "\n",
        "                output_a = netG_b2a(input_b)\n",
        "\n",
        "                # Tensorboard\n",
        "                input_b = fn_tonumpy(fn_denorm(input_b))\n",
        "                output_a = fn_tonumpy(fn_denorm(output_a))\n",
        "\n",
        "                for j in range(input_b.shape[0]):\n",
        "                    id = batch_size * (batch - 1) + j\n",
        "\n",
        "                    input_b_ = input_b[j]\n",
        "                    output_a_ = output_a[j]\n",
        "\n",
        "                    input_b_ = np.clip(input_b_, a_min=0, a_max=1)\n",
        "                    output_a_ = np.clip(output_a_, a_min=0, a_max=1)\n",
        "\n",
        "                    plt.imsave(os.path.join(result_dir_test, 'png', 'b2a', '%04d_input_b.png' % id), input_b_)\n",
        "                    plt.imsave(os.path.join(result_dir_test, 'png', 'b2a', '%04d_output_a.png' % id), output_a_)\n",
        "\n",
        "                    print(\"TEST B: BATCH %04d / %04d | \" % (id + 1, num_data_test_b))"
      ],
      "metadata": {
        "id": "PD_YAVfrYRMp"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Display Results**"
      ],
      "metadata": {
        "id": "SalfBKmMfEtI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_dir = 'define yours'\n",
        "\n",
        "lst_data = os.listdir(result_dir)\n",
        "\n",
        "lst_label = [f for f in lst_data if f.startswith('label')]\n",
        "lst_input = [f for f in lst_data if f.startswith('input')]\n",
        "lst_output = [f for f in lst_data if f.startswith('output')]\n",
        "\n",
        "lst_label.sort()\n",
        "lst_input.sort()\n",
        "lst_output.sort()\n",
        "\n",
        "##\n",
        "id = 0\n",
        "\n",
        "label = np.load(os.path.join(result_dir, lst_label[id]))\n",
        "input = np.load(os.path.join(result_dir, lst_input[id]))\n",
        "output = np.load(os.path.join(result_dir, lst_output[id]))\n",
        "\n",
        "##\n",
        "plt.subplot(131)\n",
        "plt.imshow(input, cmap='gray')\n",
        "plt.title('input')\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.imshow(label, cmap='gray')\n",
        "plt.title('label')\n",
        "\n",
        "plt.subplot(133)\n",
        "plt.imshow(output, cmap='gray')\n",
        "plt.title('output')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Z_PKPjNJiRz8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}